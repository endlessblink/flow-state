# FlowState Environment Configuration
# ====================================
#
# Option 1: Manual .env file (local development)
#   Copy this file to .env.local and fill in your values
#
# Option 2: Doppler (recommended for production)
#   See docs/sop/SOP-030-doppler-secrets-management.md
#   Run: ./scripts/setup-doppler.sh
#   Then: doppler run -- npm run dev
#
# ====================================

# Supabase Configuration (required for cloud sync)
# For LOCAL development: Run `supabase status` to get these values
# For PRODUCTION: Use Doppler (see docs/sop/SOP-030-doppler-secrets-management.md)
VITE_SUPABASE_URL=http://127.0.0.1:54321
VITE_SUPABASE_ANON_KEY=your-anon-key-from-supabase-status

# Supabase Admin (optional, for scripts only)
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-from-supabase-status

# ====================================
# AI Provider Configuration
# ====================================
#
# Architecture: Ollama (local) + Groq (cloud) + OpenRouter (fallback)
# BUG-1131: All cloud API keys are kept server-side via Supabase Edge Functions.
#
# The Edge Function at /functions/v1/ai-chat-proxy handles:
# - Groq (Llama, Mixtral, Gemma models)
# - OpenRouter (Claude, GPT-4, Llama, etc.)
#
# To configure server-side keys (REQUIRED for cloud providers):
# 1. Add GROQ_API_KEY and OPENROUTER_API_KEY to Doppler (flowstate-prod)
# 2. Deploy Supabase Edge Functions: supabase functions deploy ai-chat-proxy
# 3. NO VITE_* AI keys needed - they're all server-side now!
#
# ====================================

# Optional: Ollama Configuration (for local AI)
# Default: localhost:11434
VITE_OLLAMA_HOST=localhost
VITE_OLLAMA_PORT=11434

# Environment
NODE_ENV=development
